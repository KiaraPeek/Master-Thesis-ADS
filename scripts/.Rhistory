library(haven)
data_path <- here::here('data', 'raw', 'ari.csv')
raw_data <- read.csv(data_path)
metadata_path <- here::here('data', 'metadata', 'ari.json')
print(dim(raw_data))
ari <- raw_data
ari <- raw_data
ari$pneumonia_or_worse <- ari$cprot %in% c("Pneumonia", "Severe pneumonia", "Verey severe disease")
split_data <- function(data) {
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]
return(list(train=train, test=test))
}
train_testje, test_testje <- split_data(ari)
testje <- split_data(ari)
head(testje)
View(testje)
fit_task <- function(train, task_metadata) {
#' fit model for task to data, according to task metadata description
outcome_name <- task_metadata$outcome_variable
feature_names <- task_metadata$features
task_formula <- reformulate(feature_names, outcome_name)
# for evevery domain in domains
fit <- glm(task_formula, data=train, family = "binomial")
}
library(jsonlite)
task_metadata <- fromJSON("ari.json")
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json")
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json")
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json")
model <- fit_task(testje$train, task_metadata)
summary(ari$pneumonia_or_worse)
task_metadata
model <- fit_task(testje$train, task_metadata$tasks)
model <- fit_task(testje$train, task_metadata$tasks)
fit_task <- function(train, task_metadata) {
#' fit model for task to data, according to task metadata description
outcome_name <- task_metadata$outcome_variable
feature_names <- task_metadata$features
print(outcome_name)
print(feature_names)
task_formula <- reformulate(feature_names, outcome_name)
# for evevery domain in domains
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
model <- fit_task(testje$train, task_metadata$tasks)
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json")
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json")
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json")
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json")
fit_task <- function(train, task_metadata) {
#' fit model for task to data, according to task metadata description
outcome_name <- task_metadata$outcome_variable
feature_names <- task_metadata$features
print(outcome_name)
print(feature_names)
task_formula <- reformulate(feature_names, outcome_name)
# for evevery domain in domains
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
model <- fit_task(testje$train, task_metadata$tasks)
task_metadata <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json", simplifyVector=TRUE)
task_formula <- reformulate(feature_names, outcome_name)
fit_task <- function(train, task_metadata) {
#' fit model for task to data, according to task metadata description
outcome_name <- task_metadata$outcome_variable
feature_names <- task_metadata$features
print(outcome_name)
print(feature_names)
task_formula <- reformulate(feature_names, outcome_name)
# for evevery domain in domains
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
model <- fit_task(testje$train, task_metadata$tasks)
task_metadata$tasks$features
features <- task_metadata$tasks$features
reformulate(features, "pneumonia_or_worse")
reformulate(features, response="pneumonia_or_worse")
task_metadata
list(task_metadata$tasks$features)
unlist(task_metadata$tasks$features)
feat <- unlist(task_metadata$tasks$features)
reformulate(feat, task_metadata$tasks$outcome_variable)
fit_task <- function(train, task_metadata) {
#' fit model for task to data, according to task metadata description
outcome_name <- task_metadata$outcome_variable
feature_names <- unlist(task_metadata$features)
task_formula <- reformulate(feature_names, outcome_name)
# for evevery domain in domains
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
model <- fit_task(testje$train, task_metadata$tasks)
model
metadata_ari <- fromJSON("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json", simplifyVector=TRUE)
task_metadata <- metadata_ari$tasks
probability <- predict(model, testje$test, type='response')
head(probability)
sum(is.na(ari))
sum(is.na(ari$pneumonia_or_worse))
sum(is.na(ari$weight))
sum(is.na(ari$age))
sum(is.na(ari$rr))
test_model <- glm(task_metadata$outcome_variable ~ age + rr, data=testje$train, family='binomial')
sum(testje$train$age)
summary(testje$train$age)
summary(testje$train$rr)
dim(testje$train)
dim(testje$train$pneumonia_or_worse)
sum(is.na(testje$train$pneumonia_or_worse))
test_model <- glm(task_metadata$outcome_variable ~ integer(age) + rr, data=testje$train, family='binomial')
fit <- glm(pneumonia_or_worse ~ age + rr, data=testje$train, family="binomial")
fit
prob <- predict(fit, testje$test, type="response")
pred = rep(0, dim(testje$test)[1])
pred[am_prob > 0.5] = 1
pred[prob > 0.5] = 1
table(pred, testje$test)
table(pred, testje$test$pneumonia_or_worse)
confusionMatrix(pred, testje$test$pneumonia_or_worse)
#' this scripts fits the different models
#'
#'
#' use processed data
library(caret)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(prob_out)[1])
prediction[prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data$prob_out)[1])
prediction[test_data$prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data$prob_out)[1])
prediction[test_data$prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data$prob_out))
prediction = rep(0, dim(test_data$prob_out)[1])
prediction[test_data$prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
testje$test$prob_out
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
prob_out <- predict(fitted_model, test_data, type="response")
test_data$prob_out <- prob_out
print(dim(prob_out))
print(dim(test_data$prob_out))
prediction = rep(0, dim(test_data$prob_out)[1])
prediction[test_data$prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
prob_out <- predict(fitted_model, test_data, type="response")
print(head(prob_out))
test_data$prob_out <- prob_out
print(dim(prob_out)[1])
print(dim(test_data$prob_out))
prediction = rep(0, dim(test_data$prob_out)[1])
prediction[test_data$prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(prediction))
print(class(actual))
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- as.numeric(test_data[task_metadata$outcome_variable])
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(prediction))
print(class(actual))
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
summary(testje$test$pneumonia_or_worse)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- as.integer(test_data[task_metadata$outcome_variable])
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(prediction))
print(class(actual))
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
test_data[task_metadata$outcome_variable] <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(prediction))
print(class(actual))
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
test_data[task_metadata$outcome_variable] <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
print(head(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual))
print(class(prediction))
confusionMatrix(prediction, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
test_data[task_metadata$outcome_variable] <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
print(head(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual))
print(class(prediction))
confusionMatrix(as.data.frame(prediction), actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
test_data[task_metadata$outcome_variable] <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
print(head(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual))
print(class(prediction))
print(class(as.data.frame(prediction)))
confusionMatrix(as.data.frame(prediction), actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
test_data[task_metadata$outcome_variable] <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual <- test_data[task_metadata$outcome_variable]
print(dim(actual))
print(head(actual))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual))
print(class(prediction))
df_pred <- as.data.frame(prediction)
print(class(df_pred))
print(dim(df_pred))
confusionMatrix(as.data.frame(prediction), actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- lapply(test_data[task_metadata$outcome_variable], as.integer)
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual))
print(class(prediction))
df_pred <- as.data.frame(prediction)
print(class(df_pred))
print(dim(df_pred))
confusionMatrix(df_pred, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- lapply(test_data[task_metadata$outcome_variable], as.integer)
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual))
print(class(prediction))
confusionMatrix(df_pred, actual)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
prediction[test_data$prob_out > 0.5] = 1
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual_num <- unlist(actual)
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual_num))
print(class(prediction))
confusionMatrix(df_pred, actual_num)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual_num <- unlist(actual)
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual_num))
print(class(prediction))
confusionMatrix(prediction, actual_num)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
evaluate_model <- function(fitted_model, test_data, task_metadata) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- lapply(test_data[task_metadata$outcome_variable], as.integer)
actual_num <- unlist(actual)
test_data$prob_out <- predict(fitted_model, test_data, type="response")
print(dim(test_data)[1])
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
print(class(actual_num))
print(class(prediction))
actual_factor <- factor(actual_num, levels=c(0,1))
prediction_factor <- factor(prediction, levels=c(0,1))
confusionMatrix(prediction_factor, actual_factor)
}
matrix <- evaluate_model(fit, testje$test, task_metadata)
matrix
library(ROCR)
library(pROC)
# Example data
predicted <- c(0.2, 0.7, 0.4, 0.6, 0.9)
actual <- c(0, 1, 0, 1, 1)
# Create prediction object
pred <- prediction(predicted, actual)
auc(actual, predicted)
library(CRAN)
roc(actual, predicted, plot=TRUE)
auc(actual, predicted, plot=TRUE)
roc(actual, predicted, plot=TRUE, legacy.axes=TRUE)
par(pty='s')
roc(actual, predicted, plot=TRUE, legacy.axes=TRUE)
set.seed(123)
# Generate predicted probabilities from a uniform distribution
predicted <- runif(1000)
# Generate corresponding true labels, biased towards the predicted probabilities
actual <- rbinom(1000, size = 1, prob = predicted)
par(pty='s')
roc(actual, predicted, plot=TRUE, legacy.axes=TRUE)
roc(actual, predicted, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
library(ggplot2)
cm <- conf_mat(actual, predicted)
library(yardstick)
install.packages('yardstick')
library(yardstick)
cm <- conf_mat(actual, predicted)
set.seed(123)
truth_predicted <- data.frame(
obs = sample(0:1,100, replace = T),
pred = sample(0:1,100, replace = T)
)
truth_predicted$obs <- as.factor(truth_predicted$obs)
truth_predicted$pred <- as.factor(truth_predicted$pred)
cm <- conf_mat(truth_predicted, obs, pred)
autoplot(cm, type = "heatmap") +
scale_fill_gradient(low = "pink", high = "cyan")
