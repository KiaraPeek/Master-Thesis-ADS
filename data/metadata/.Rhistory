for (i in 1:nrow(task_test)) { print(task_test$features[[i]])}
task_test
task_test[1]
task_test[,1]
task_test[1,]
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_test)) {
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[[i]] <- df_model
}
#' This scripts fits the different models
#'
#'
#'
suppressWarnings(library(caret))
suppressWarnings(library(jsonlite))
suppressWarnings(library(yardstick))
suppressWarnings(library(pROC))
suppressWarnings(library(CalibrationCurves))
# library(dplyr)
split_data <- function(data) {
#' This function splits the data into a training and test set (70%,30%) and returns them as a list
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]
return(list(train=train, test=test))
}
fit_task <- function(train, task) {
#' This function fits a linear model based on the task metadata description and returns the model
outcome_name <- task$outcome_variable
feature_names <- unlist(task$features)
task_formula <- reformulate(feature_names, outcome_name)
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
evaluate_model <- function(fitted_model, train_data, test_data, task, metadata) {
#' Metrics: AUC, calibration
#' Plots: ROC-plot, calibration plot
#'
# Get the outcome variable
outcome_var <- task$outcome_variable
# Get the actual result
actual_train <- as.integer(unlist(train_data[outcome_var])) #unlist(train_data$outcome_var)
actual_test <- as.integer(unlist(test_data[outcome_var])) #unlist(test_data$outcome_var)
# Get the predicted probabilities of the model based on the data
probability_train <- predict(fitted_model, type="response")
probability_test <- predict(fitted_model, test_data, type="response")
# Create a confusion matrix plot
# conf_plot <- create_conf_plot(actual, probability)
# Plot the roc curves for the probabilities and the actual values
roc_curve_train <- roc(actual_train, probability_train, plot=FALSE, levels=c(0,1))
roc_curve_test <- roc(actual_test, probability_test, plot=FALSE, levels=c(0,1))
# Get the AUC values
auc_value_train <- auc(roc_curve_train)
auc_value_test <- auc(roc_curve_test)
# Save ROC plot and calibration plot in a .png file, store calibration values
save_roc_curve(roc_curve_train, roc_curve_test, task, metadata$tag)
cal_values <- save_cal(probability_train, probability_test, actual_train, actual_test, task, metadata$tag)
# Save variables: auc_value, intercept, slope
df_eval <- data.frame(dataset = metadata$full_name,
task = task$type,
test_environment = "test", # change this in .json file
AUC_train = auc_value_train[1],
AUC_test = auc_value_test[1],
calibration_train = list(int=cal_values$calibration_train.intercept, slope=cal_values$calibration_train.slope),
calibration_test = list(int=cal_values$calibration_test.intercept, slope=cal_values$calibration_test.slope),
N_train = nrow(train_data),
N_test = nrow(test_data),
num_features = length(task$features[[1]]))
df_eval
}
# -- Is not necessary --
#' create_conf_plot <- function(actual, probability) {
#'   #' For the confusion matrix, first get the predictions based on a threshold (0.5)
#'   #' Create a dataframe of actual values and predicted values
#'   #' Plot them in a graph
#'   predictions <- ifelse(probability > 0.5, 1, 0)
#'   conf_df <- as.data.frame.table(table(actual, predictions))
#'   colnames(conf_df) <- c("Actual", "Predicted", "Count")
#'
#'   # Plot the confusion matrix for the threshold
#'   conf_plot <- ggplot(data = conf_df, aes(x = Actual, y = Predicted, fill = Count)) +
#'     geom_tile(color = "white") +
#'     geom_text(aes(label = Count), vjust = 1) +
#'     scale_fill_gradient(low = "hotpink", high = "lightgreen") +
#'     theme_minimal() +
#'     labs(x = "Actual", y = "Predicted", title = "Confusion Matrix")
#' }
save_roc_curve <- function(roc_curve_train, roc_curve_test, task, dataset_name) {
# Save the roc curve as a .png file
# Location!!
png(paste(dataset_name,"_roc_", task$type, "_", task$number, ".png", sep=""))
plot(roc_curve_train, main="ROC Curve", print.auc = TRUE, print.auc.y = 0.4, legacy.axes=TRUE, col='blue', lwd=2)
plot(roc_curve_test, print.auc = TRUE, print.auc.y = 0.4, legacy.axes=TRUE, col='purple', lwd=2, add=TRUE)
dev.off()
}
save_cal <- function(probability_train, probability_test, actual_train, actual_test, task, dataset_name) {
# Save the calibration curve as a .png file
# Location!!
png(paste(dataset_name, "_cal_train_", task$type, "_", task$number, ".png", sep=""))
cal_train = val.prob.ci.2(probability_train, actual_train, logistic.cal=TRUE, smooth="none", col.log = "blue")
dev.off()
png(paste(dataset_name, "_cal_test_", task$type, "_", task$number, ".png", sep=""))
cal_test = val.prob.ci.2(probability_test, actual_test, logistic.cal=TRUE, smooth="none", col.log = "purple")
dev.off()
incpt_train <- round(cal_train$Calibration$Intercept[[1]],2)
slope_train <- round(cal_train$Calibration$Slope[[1]],2)
incpt_test <- round(cal_test$Calibration$Intercept[[1]],2)
slope_test <- round(cal_test$Calibration$Slope[[1]],2)
cal_values <- data.frame(calibration_train = list("intercept"=incpt_train, "slope"=slope_train),
calibration_test = list("intercept"=incpt_test, "slope"=slope_test))
cal_values
}
# Run the script with this function:
analysis <- function() {
# Get the processed data
path_processed_data <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/processed/processed_ari.csv'
processed_data <- read.csv(path_processed_data)
# Split the data into training and test set
split <- split_data(processed_data)
# Get the task from the metadata
path_metadata <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json'
metadata_ari <- fromJSON(path_metadata, simplifyVector=TRUE)
task_metadata <- as.data.frame(metadata_ari$tasks) # this is a list of tasks
path_test <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/test.json'
meta_test <- fromJSON(path_test, simplifyVector = TRUE)
task_test <- as.data.frame(meta_test$tasks)
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_test)) {
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[[i]] <- df_model
}
# Store the values in a dataframe / .csv file
# Location!
results <- rbind(row_list)
write.csv(results, file=paste("results_", metadata_ari$tag,".csv"), row_names=FALSE)
}
# Get the processed data
path_processed_data <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/processed/processed_ari.csv'
processed_data <- read.csv(path_processed_data)
View(processed_data)
colSums(is.na(processed_data))
# Split the data into training and test set
split <- split_data(processed_data)
path_test <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/test.json'
meta_test <- fromJSON(path_test, simplifyVector = TRUE)
task_test <- as.data.frame(meta_test$tasks)
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_test)) {
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[[i]] <- df_model
}
for (i in 1:nrow(task_test)) {
print(i)
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[[i]] <- df_model
}
path_test <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/test.json'
meta_test <- fromJSON(path_test, simplifyVector = TRUE)
task_test <- as.data.frame(meta_test$tasks)
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_test)) {
print(i)
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[[i]] <- df_model
}
head(row_list)
# Store the values in a dataframe / .csv file
# Location!
results <- rbind(row_list)
View(results)
# Store the values in a dataframe / .csv file
# Location!
write.csv(row_list, file=paste("results_", metadata_ari$tag,".csv"), row_names=FALSE)
# Store the values in a dataframe / .csv file
# Location!
write.csv(row_list, file=paste("results_", metadata_ari$tag,".csv"), row.names=FALSE)
# Store the values in a dataframe / .csv file
# Location!
write.csv(row_list, file=paste("results_", metadata_ari$tag,".csv", sep=""), row.names=FALSE)
row_list
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_test)) {
print(i)
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[i] <- df_model
}
row_list
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_test)) {
print(i)
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[[i]] <- df_model
}
unlist(row_list)
row_list
combined_df <- do.call(rbind, row_list)
combined_df
# Store the values in a dataframe / .csv file
# Location!
write.csv(combined_df, file=paste("results_", metadata_ari$tag,".csv", sep=""), row.names=FALSE)
#' This scripts fits the different models
#'
#'
#'
suppressWarnings(library(caret))
suppressWarnings(library(jsonlite))
suppressWarnings(library(yardstick))
suppressWarnings(library(pROC))
suppressWarnings(library(CalibrationCurves))
# library(dplyr)
split_data <- function(data) {
#' This function splits the data into a training and test set (70%,30%) and returns them as a list
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]
return(list(train=train, test=test))
}
fit_task <- function(train, task) {
#' This function fits a linear model based on the task metadata description and returns the model
outcome_name <- task$outcome_variable
feature_names <- unlist(task$features)
task_formula <- reformulate(feature_names, outcome_name)
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
evaluate_model <- function(fitted_model, train_data, test_data, task, metadata) {
#' Metrics: AUC, calibration
#' Plots: ROC-plot, calibration plot
#'
# Get the outcome variable
outcome_var <- task$outcome_variable
# Get the actual result
actual_train <- as.integer(unlist(train_data[outcome_var])) #unlist(train_data$outcome_var)
actual_test <- as.integer(unlist(test_data[outcome_var])) #unlist(test_data$outcome_var)
# Get the predicted probabilities of the model based on the data
probability_train <- predict(fitted_model, type="response")
probability_test <- predict(fitted_model, test_data, type="response")
# Create a confusion matrix plot
# conf_plot <- create_conf_plot(actual, probability)
# Plot the roc curves for the probabilities and the actual values
roc_curve_train <- roc(actual_train, probability_train, plot=FALSE, levels=c(0,1))
roc_curve_test <- roc(actual_test, probability_test, plot=FALSE, levels=c(0,1))
# Get the AUC values
auc_value_train <- auc(roc_curve_train)
auc_value_test <- auc(roc_curve_test)
# Save ROC plot and calibration plot in a .png file, store calibration values
save_roc_curve(roc_curve_train, roc_curve_test, task, metadata$tag)
cal_values <- save_cal(probability_train, probability_test, actual_train, actual_test, task, metadata$tag)
# Save variables: auc_value, intercept, slope
df_eval <- data.frame(dataset = metadata$full_name,
task = task$type,
test_environment = metadata$test_environment,
AUC_train = auc_value_train[1],
AUC_test = auc_value_test[1],
calibration_train = list(int=cal_values$calibration_train.intercept, slope=cal_values$calibration_train.slope),
calibration_test = list(int=cal_values$calibration_test.intercept, slope=cal_values$calibration_test.slope),
N_train = nrow(train_data),
N_test = nrow(test_data),
num_features = length(task$features[[1]]))
df_eval
}
# -- Is not necessary --
#' create_conf_plot <- function(actual, probability) {
#'   #' For the confusion matrix, first get the predictions based on a threshold (0.5)
#'   #' Create a dataframe of actual values and predicted values
#'   #' Plot them in a graph
#'   predictions <- ifelse(probability > 0.5, 1, 0)
#'   conf_df <- as.data.frame.table(table(actual, predictions))
#'   colnames(conf_df) <- c("Actual", "Predicted", "Count")
#'
#'   # Plot the confusion matrix for the threshold
#'   conf_plot <- ggplot(data = conf_df, aes(x = Actual, y = Predicted, fill = Count)) +
#'     geom_tile(color = "white") +
#'     geom_text(aes(label = Count), vjust = 1) +
#'     scale_fill_gradient(low = "hotpink", high = "lightgreen") +
#'     theme_minimal() +
#'     labs(x = "Actual", y = "Predicted", title = "Confusion Matrix")
#' }
save_roc_curve <- function(roc_curve_train, roc_curve_test, task, dataset_name) {
# Save the roc curve as a .png file
# Location!!
png(paste(dataset_name, "_", task$number, "_roc_", task$type, ".png", sep=""))
plot(roc_curve_train, main="ROC Curve", print.auc = TRUE, print.auc.y = 0.4, legacy.axes=TRUE, col='blue', lwd=2)
plot(roc_curve_test, print.auc = TRUE, print.auc.y = 0.3, legacy.axes=TRUE, col='purple', lwd=2, add=TRUE)
dev.off()
}
save_cal <- function(probability_train, probability_test, actual_train, actual_test, task, dataset_name) {
# Save the calibration curve as a .png file
# Location!!
png(paste(dataset_name, "_", task$number, "_cal_train_", task$type, ".png", sep=""))
cal_train = val.prob.ci.2(probability_train, actual_train, logistic.cal=TRUE, smooth="none", col.log = "blue")
dev.off()
png(paste(dataset_name, "_", task$number, "_cal_test_", task$type, ".png", sep=""))
cal_test = val.prob.ci.2(probability_test, actual_test, logistic.cal=TRUE, smooth="none", col.log = "purple")
dev.off()
incpt_train <- round(cal_train$Calibration$Intercept[[1]],2)
slope_train <- round(cal_train$Calibration$Slope[[1]],2)
incpt_test <- round(cal_test$Calibration$Intercept[[1]],2)
slope_test <- round(cal_test$Calibration$Slope[[1]],2)
cal_values <- data.frame(calibration_train = list("intercept"=incpt_train, "slope"=slope_train),
calibration_test = list("intercept"=incpt_test, "slope"=slope_test))
cal_values
}
# Run the script with this function:
analysis <- function() {
# Get the processed data
path_processed_data <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/processed/processed_ari.csv'
processed_data <- read.csv(path_processed_data)
# Split the data into training and test set
split <- split_data(processed_data)
# Get the task from the metadata
path_metadata <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json'
metadata_ari <- fromJSON(path_metadata, simplifyVector=TRUE)
task_metadata <- as.data.frame(metadata_ari$tasks) # this is a list of tasks
path_test <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/test.json'
meta_test <- fromJSON(path_test, simplifyVector = TRUE)
task_test <- as.data.frame(meta_test$tasks)
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_test)) {
print(i)
model <- fit_task(split$train, task_test[i,]) #change to task_metadata when deleting for-loop
df_model <- evaluate_model(model, split$train, split$test, task_test[i,], meta_test) #change to task when adding for-loop
row_list[[i]] <- df_model
}
# Store the values in a .csv file
# Location!
combined_df <- do.call(rbind, row_list)
write.csv(combined_df, file=paste("results_", metadata_ari$tag,".csv", sep=""), row.names=FALSE)
}
analysis()
# Run the script with this function:
analysis <- function() {
# Get the processed data
path_processed_data <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/processed/processed_ari.csv'
processed_data <- read.csv(path_processed_data)
# Split the data into training and test set
split <- split_data(processed_data)
# Get the task from the metadata
path_metadata <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json'
metadata_ari <- fromJSON(path_metadata, simplifyVector=TRUE)
task_metadata <- as.data.frame(metadata_ari$tasks) # this is a list of tasks
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_metadata)) {
print(i)
model <- fit_task(split$train, task_metadata[i,])
df_model <- evaluate_model(model, split$train, split$test, task_metadata[i,], metadata_ari)
row_list[[i]] <- df_model
}
# Store the values in a .csv file
# Location!
combined_df <- do.call(rbind, row_list)
write.csv(combined_df, file=paste("results_", metadata_ari$tag,".csv", sep=""), row.names=FALSE)
}
analysis()
#' This scripts fits the different models
#'
#'
#'
suppressWarnings(library(caret))
suppressWarnings(library(jsonlite))
suppressWarnings(library(yardstick))
suppressWarnings(library(pROC))
suppressWarnings(library(CalibrationCurves))
# library(dplyr)
split_data <- function(data) {
#' This function splits the data into a training and test set (70%,30%) and returns them as a list
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]
return(list(train=train, test=test))
}
fit_task <- function(train, task) {
#' This function fits a linear model based on the task metadata description and returns the model
outcome_name <- task$outcome_variable
feature_names <- unlist(task$features)
task_formula <- reformulate(feature_names, outcome_name)
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
evaluate_model <- function(fitted_model, train_data, test_data, task, metadata) {
#' Metrics: AUC, calibration
#' Plots: ROC-plot, calibration plot
#'
# Get the outcome variable
outcome_var <- task$outcome_variable
# Get the actual result
actual_train <- as.integer(unlist(train_data[outcome_var])) #unlist(train_data$outcome_var)
actual_test <- as.integer(unlist(test_data[outcome_var])) #unlist(test_data$outcome_var)
# Get the predicted probabilities of the model based on the data
probability_train <- predict(fitted_model, type="response")
probability_test <- predict(fitted_model, test_data, type="response")
# Create a confusion matrix plot
# conf_plot <- create_conf_plot(actual, probability)
# Plot the roc curves for the probabilities and the actual values
roc_curve_train <- roc(actual_train, probability_train, plot=FALSE, levels=c(0,1))
roc_curve_test <- roc(actual_test, probability_test, plot=FALSE, levels=c(0,1))
# Get the AUC values
auc_value_train <- auc(roc_curve_train)
auc_value_test <- auc(roc_curve_test)
# Save ROC plot and calibration plot in a .png file, store calibration values
setwd('G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/graphics')
save_roc_curve(roc_curve_train, roc_curve_test, task, metadata$tag)
cal_values <- save_cal(probability_train, probability_test, actual_train, actual_test, task, metadata$tag)
# Save variables: auc_value, intercept, slope
df_eval <- data.frame(dataset = metadata$full_name,
task = task$type,
test_environment = metadata$test_environment,
AUC_train = auc_value_train[1],
AUC_test = auc_value_test[1],
calibration_train = list(int=cal_values$calibration_train.intercept, slope=cal_values$calibration_train.slope),
calibration_test = list(int=cal_values$calibration_test.intercept, slope=cal_values$calibration_test.slope),
N_train = nrow(train_data),
N_test = nrow(test_data),
num_features = length(task$features[[1]]))
df_eval
}
# -- Is not necessary --
#' create_conf_plot <- function(actual, probability) {
#'   #' For the confusion matrix, first get the predictions based on a threshold (0.5)
#'   #' Create a dataframe of actual values and predicted values
#'   #' Plot them in a graph
#'   predictions <- ifelse(probability > 0.5, 1, 0)
#'   conf_df <- as.data.frame.table(table(actual, predictions))
#'   colnames(conf_df) <- c("Actual", "Predicted", "Count")
#'
#'   # Plot the confusion matrix for the threshold
#'   conf_plot <- ggplot(data = conf_df, aes(x = Actual, y = Predicted, fill = Count)) +
#'     geom_tile(color = "white") +
#'     geom_text(aes(label = Count), vjust = 1) +
#'     scale_fill_gradient(low = "hotpink", high = "lightgreen") +
#'     theme_minimal() +
#'     labs(x = "Actual", y = "Predicted", title = "Confusion Matrix")
#' }
save_roc_curve <- function(roc_curve_train, roc_curve_test, task, dataset_name) {
# Save the roc curve as a .png file
# Location!!
png(paste(dataset_name, "_", task$number, "_roc_", task$type, ".png", sep=""))
plot(roc_curve_train, main="ROC Curve", print.auc = TRUE, print.auc.y = 0.4, legacy.axes=TRUE, col='blue', lwd=2)
plot(roc_curve_test, print.auc = TRUE, print.auc.y = 0.3, legacy.axes=TRUE, col='purple', lwd=2, add=TRUE)
dev.off()
}
save_cal <- function(probability_train, probability_test, actual_train, actual_test, task, dataset_name) {
# Save the calibration curve as a .png file
# Location!!
png(paste(dataset_name, "_", task$number, "_cal_train_", task$type, ".png", sep=""))
cal_train = val.prob.ci.2(probability_train, actual_train, logistic.cal=TRUE, smooth="none", col.log = "blue")
dev.off()
png(paste(dataset_name, "_", task$number, "_cal_test_", task$type, ".png", sep=""))
cal_test = val.prob.ci.2(probability_test, actual_test, logistic.cal=TRUE, smooth="none", col.log = "purple")
dev.off()
incpt_train <- round(cal_train$Calibration$Intercept[[1]],2)
slope_train <- round(cal_train$Calibration$Slope[[1]],2)
incpt_test <- round(cal_test$Calibration$Intercept[[1]],2)
slope_test <- round(cal_test$Calibration$Slope[[1]],2)
cal_values <- data.frame(calibration_train = list("intercept"=incpt_train, "slope"=slope_train),
calibration_test = list("intercept"=incpt_test, "slope"=slope_test))
cal_values
}
# Run the script with this function:
analysis <- function() {
# Get the processed data
path_processed_data <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/processed/processed_ari.csv'
processed_data <- read.csv(path_processed_data)
# Split the data into training and test set
split <- split_data(processed_data)
# Get the task from the metadata
path_metadata <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json'
metadata_ari <- fromJSON(path_metadata, simplifyVector=TRUE)
task_metadata <- as.data.frame(metadata_ari$tasks) # this is a list of tasks
# For each task, fit and evaluate the model
row_list <- list()
for (i in 1:nrow(task_metadata)) {
print(i)
model <- fit_task(split$train, task_metadata[i,])
df_model <- evaluate_model(model, split$train, split$test, task_metadata[i,], metadata_ari)
row_list[[i]] <- df_model
}
# Store the values in a .csv file
# Location!
setwd('G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata')
combined_df <- do.call(rbind, row_list)
write.csv(combined_df, file=paste("results_", metadata_ari$tag,".csv", sep=""), row.names=FALSE)
}
analysis()
library(data.table)
library(ggplot2); theme_set(theme_bw())
library(haven)
dystonia <- read_dta(here::here("data", "raw", "cdystonia.dta"))
setDT(dystonia)
dystonia[, id2:=paste0('site', site, 'id', id)]
print(dystonia[, uniqueN(id2)])
dystonia
dystonia[week==0]
