#'
actual <- lapply(test_data[task$outcome_variable], as.integer)
actual_num <- unlist(actual)
# print(class(actual_num))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
# print(head(prediction))
actual_factor <- factor(actual_num, levels=c(0,1))
prediction_factor <- factor(prediction, levels=c(0,1))
# print(class(actual_factor))
# print(class(prediction))
cm <- conf_mat(actual_factor, prediction_factor)
autoplot(cm, type="heatmap") +
scale_fill_gradient(low="hotpink", high="lightgreen")
roc(actual, prediction, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
}
x <- evaluate_model(model, split$test, task$tasks)
actual_factor
library(yardstick)
test_actual <- sample(c(0, 1), 10, replace = TRUE)
test_predict <- sample(c(0,1), 10, replace = TRUE)
conf_mat(test_actual, test_predict)
test_actual <- as.factor(sample(c(0, 1), 10, replace = TRUE))
conf_mat(test_actual, test_predict)
evaluate_model <- function(fitted_model, test_data, task) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- lapply(test_data[task$outcome_variable], as.integer)
actual_num <- unlist(actual)
# print(class(actual_num))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
# print(head(prediction))
actual_factor <- factor(actual_num, levels=c(0,1))
prediction_factor <- factor(prediction, levels=c(0,1))
conf_table <- table(
actual = actual_factor,
predict = predicton_factor
)
# print(class(actual_factor))
# print(class(prediction))
cm <- conf_mat(conf_table, actual, predict)
autoplot(cm, type="heatmap") +
scale_fill_gradient(low="hotpink", high="lightgreen")
roc(conf_table$actual, conf_table$predict, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
}
x <- evaluate_model(model, split$test, task$tasks)
fit <- glm(task_formula, data=train, family = "binomial")
evaluate_model <- function(fitted_model, test_data, task) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
actual <- lapply(test_data[task$outcome_variable], as.integer)
actual_num <- unlist(actual)
# print(class(actual_num))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
# print(head(prediction))
actual_factor <- factor(actual_num, levels=c(0,1))
prediction_factor <- factor(prediction, levels=c(0,1))
conf_table <- table(
actual = actual_factor,
predict = prediction_factor
)
# print(class(actual_factor))
# print(class(prediction))
cm <- conf_mat(conf_table, actual, predict)
autoplot(cm, type="heatmap") +
scale_fill_gradient(low="hotpink", high="lightgreen")
roc(conf_table$actual, conf_table$predict, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
}
x <- evaluate_model(model, split$test, task$tasks)
test_table <- table(actual = test_actual, predict = test_predict)
test_table[actual]
conf_mat(data = data.frame(truth = test_actual, estimate = test_predict))
conf_mat(truth = test_actual, estimate = test_predict)
test_actual <- sample(c(0, 1), 10, replace = TRUE)
conf_mat(truth = test_actual, estimate = test_predict)
conf_mat(data=data.frame(actual=test_actual, predict=test_predict))
conf_mat(data=data.frame(truth=test_actual, estimate=test_predict))
conf_mat(data=data.frame(truth=test_actual, estimate=test_predict), test_actual, test_predict)
calibration(data.frame(test_predict, test_actual), "")
calibration(data.frame(test_predict, test_actual))
set.seed(123)
data <- iris
train_indices <- createDataPartition(data$Species, p = 0.8, list = FALSE)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Train a model (e.g., random forest) using caret
library(caret)
library(yardstick)
fit <- train(Species ~ ., data = train_data, method = "rf")
# Predict on test set
predictions <- predict(fit, newdata = test_data, type = "prob")
# Extract predicted probabilities for class 1
predicted_probabilities <- predictions[, "versicolor"]
# Extract actual labels for test set
actual_labels <- ifelse(test_data$Species == "versicolor", 1, 0)
# Create a data frame with predicted probabilities and actual labels
calibration_data <- data.frame(predicted_probabilities, actual_labels)
# Create calibration plot
calibration_plot <- calibration(calibration_data, "versicolor")  # Specify the class of interest
calibration_plot <- yardstick::calibration(calibration_data, "versicolor")
library(calibrate)
install.packages("CalibrationCurves")
library(CalibrationCurves)
data("traindata")
head(traindata)
glmFit = glm(y ~ . , data = traindata, family = binomial)
summary(glmFit)
data("testdata")
pHat = predict(glmFit, newdata = testdata, type = "response")
yTest = testdata$y
calPerf = val.prob.ci.2(pHat, yTest)
prediction[test_data$prob_out > 0.5] = 1
#' this scripts fits the different models
#'
#'
#' use processed data
library(caret)
library(jsonlite)
library(yardstick)
library(pROC)
# library(dplyr)
split_data <- function(data) {
#' This function splits the data into a training and test set (70%,30%) and returns a list of two
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]
return(list(train=train, test=test))
}
fit_task <- function(train, task) {
#' This function fits a linear model based on the task metadata description and returns the model
outcome_name <- task$outcome_variable
feature_names <- unlist(task$features)
task_formula <- reformulate(feature_names, outcome_name)
# for evevery domain in domains
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
evaluate_model <- function(fitted_model, test_data, task) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
outcome_var <- task$outcome_variable
class(outcome_var)
actual <- test_data$outcome_var
class(actual)
# actual <- lapply(test_data[task$outcome_variable], as.integer)
# actual_num <- unlist(actual)
# print(class(actual_num))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
# print(head(prediction))
actual_factor <- factor(actual_num, levels=c(0,1))
prediction_factor <- factor(prediction, levels=c(0,1))
conf_table <- table(
actual = actual_factor,
predict = prediction_factor
)
# print(class(actual_factor))
# print(class(prediction))
cm <- conf_mat(conf_table, actual, predict) # ERROR
autoplot(cm, type="heatmap") +
scale_fill_gradient(low="hotpink", high="lightgreen")
roc(actual_factor, prediction_factor, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
# calibration(test_data, outcome='')
}
data_path <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/raw/ari.csv'
raw_data <- read.csv(data_path)
ari <- raw_data
ari$pneumonia_or_worse <- as.numeric(ari$cprot %in% c("Pneumonia", "Severe pneumonia", "Very severe disease"))
split <- split_data(ari)
task <- fromJSON('{"tag": "ari",
+ "full_name": "ARI",
+ "url": "https://hbiostat.org/data/repo/ari.zip",
+ "tasks": [
+ {"outcome_variable": "pneumonia_or_worse",
+  "type": "diagnosis",
+  "features": ["age", "rr"] #removed weight cause it has a lot of missing values
+ }
+ ]
+ }')
task <- fromJSON('{"tag": "ari",
"full_name": "ARI",
"url": "https://hbiostat.org/data/repo/ari.zip",
"tasks": [
{"outcome_variable": "pneumonia_or_worse",
"type": "diagnosis",
"features": ["age", "rr"] #removed weight cause it has a lot of missing values
}
]
}')
task <- fromJSON('{"tag": "ari",
"full_name": "ARI",
"url": "https://hbiostat.org/data/repo/ari.zip",
"tasks": [
{"outcome_variable": "pneumonia_or_worse",
"type": "diagnosis",
"features": ["age", "rr"]
}
]
}')
model <- fit_task(split$train, task$tasks)
x <- evaluate_model(model, split$test, task$tasks)
evaluate_model <- function(fitted_model, test_data, task) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
outcome_var <- task$outcome_variable
class(outcome_var)
actual <- test_data$outcome_var
class(actual)
# actual <- lapply(test_data[task$outcome_variable], as.integer)
# actual_num <- unlist(actual)
# print(class(actual_num))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
# print(head(prediction))
actual_factor <- factor(actual, levels=c(0,1))
prediction_factor <- factor(prediction, levels=c(0,1))
conf_table <- table(
actual = actual_factor,
predict = prediction_factor
)
# print(class(actual_factor))
# print(class(prediction))
cm <- conf_mat(conf_table, actual, predict) # ERROR
autoplot(cm, type="heatmap") +
scale_fill_gradient(low="hotpink", high="lightgreen")
roc(actual_factor, prediction_factor, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
# calibration(test_data, outcome='')
}
x <- evaluate_model(model, split$test, task$tasks)
evaluate_model <- function(fitted_model, test_data, task) {
#' outputs:
#' metrics: AUC, calibration
#' plots: ROC-plot, calibration plot
#'
outcome_var <- task$outcome_variable
class(outcome_var)
actual <- test_data$outcome_var
class(actual)
# actual <- lapply(test_data[task$outcome_variable], as.integer)
# actual_num <- unlist(actual)
# print(class(actual_num))
test_data$prob_out <- predict(fitted_model, test_data, type="response")
prediction = rep(0, dim(test_data)[1])
prediction[test_data$prob_out > 0.5] = 1
# print(head(prediction))
# actual_factor <- factor(actual, levels=c(0,1))
# prediction_factor <- factor(prediction, levels=c(0,1))
# conf_table <- table(
#   actual = actual_factor,
#   predict = prediction_factor
# )
# print(class(actual_factor))
# print(class(prediction))
# cm <- conf_mat(conf_table, actual, predict) # ERROR
# autoplot(cm, type="heatmap") +
#   scale_fill_gradient(low="hotpink", high="lightgreen")
#
# roc(actual_factor, prediction_factor, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
#
# calibration(test_data, outcome='')
}
x <- evaluate_model(model, split$test, task$tasks)
class(task$tasks$outcome_variable)
act <- split$test[task$tasks$outcome_variable,]
act <- split$test[task$tasks$outcome_variable]
predictions <- predict(model, split$test, type='response')
threshold <- 0.5
binary_pred <- ifelse(predictions > threshold, 1, 0)
unlist(act)
act2 <- unlist(act)
conf_mat(act2, binary_pred)
conf <- table(act2, binary_pred)
autoplot(conf, type="heatmap") + scale_fill_gradient(low="hotpink", high="lightgreen")
conf_df <- as.data.frame.table(conf)
colnames(conf_df) <- c("Actual", "Predicted", "Count")
ggplot(data = conf_df, aes(x = Actual, y = Predicted, fill = Count)) +
geom_tile(color = "white") +
geom_text(aes(label = Count), vjust = 1) +
scale_fill_gradient(low = "hotpink", high = "lightgreen") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", title = "Confusion Matrix")
roc(act2, binary_pred, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
library(CalibrationCurves)
calPerf = val.prob.ci.2(predictions, act)
calPerf = val.prob.ci.2(binary_pred, act)
calPerf = val.prob.ci.2(binary_pred, act2)
roc(act2, predictions, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
head(ari$rr)
summary(ari$rr)
summary(ari$age)
roc_curv <- roc(act2, predictions, plot=TRUE, legacy.axes=TRUE, col='blue', lwd=3)
auc_val <- auc(roc_curv)
plot(roc_curv, main=paste("ROC Curve (AUC=", round(auc_value, 3), ")"))
plot(roc_curv, main=paste("ROC Curve (AUC=", round(auc_val, 3), ")"))
plot(roc_curv, main=paste("ROC Curve (AUC=", round(auc_val, 3), ")"), legacy.axes=TRUE, col='aquamarine', lwd=3)
calPerf = val.prob.ci.2(predictions, act2)
head(split$test$task$outcome_variable)
head(split$test[task$outcome_variable])
head(split$test)
head(split$test[task$tasks$outcome_variable])
y <- task$tasks$outcome_variable
modelc <- glm(y ~ ., data=split$train, family='binomial')
ari_na <- na.omit(ari)
split_na <- split_data(ari_na)
modelc <- glm(y ~ ., data=split_na$train, family='binomial')
str(split_na$train)
summary(split_na$train$pneumonia_or_worse)
count(split_na$train$pneumonia_or_worse)
View(calPerf)
calPerf$Calibration$Intercept
calPerf$Calibration$Slope
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, col.log="orange1"))
val.prob.ci.2(predictions, act2, logistic.cal=TRUE, col.log="orange1")
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, col.log="orange1", lwd.log = 2, lwd.smooth = 2))
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, col.log="orange1", lwd.ideal=2))
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, col.log="orange1", lwd.log=1.5))
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, col.log="blue"))
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, col.log="blue", col.smooth = "orange"))
calPerf$Calibration$Intercept[0]
calPerf$Calibration$Intercept[1]
calPerf$Calibration$Intercept[[1]]
round(calPerf$Calibration$Intercept[[1]],3)
round(calPerf$Calibration$Slope[[1]],3)
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, flexible.cal=FALSE, col.log="blue", col.smooth = "orange"))
warnings9)
warnings()
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, smooth = FALSE, col.log="blue", col.smooth = "orange"))
invisible(val.prob.ci.2(predictions, act2, logistic.cal=TRUE, smooth = "none", col.log="blue", col.smooth = "orange"))
getwd(0)
getwd()
setwd("G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/graphics")
task <- fromJSON('{"tag": "ari",
"full_name": "ARI",
"url": "https://hbiostat.org/data/repo/ari.zip",
"tasks": [
{"number": "1",
"outcome_variable": "pneumonia_or_worse",
"type": "diagnosis",
"features": ["age", "rr"]
}
]
}')
png(paste("roc_task_", task$number, ".png"))
plot(roc_curv, main=paste("ROC Curve (AUC=", round(auc_val, 3),")"), legacy.axes=TRUE, col='blue', lwd=3)
dev.off()
png(paste("roc_task_", task$tasks$number, ".png"))
plot(roc_curv, main=paste("ROC Curve (AUC=", round(auc_val, 3),")"), legacy.axes=TRUE, col='blue', lwd=3)
dev.off()
png(paste("roc_task_", task$tasks$type, "_", task$tasks$number, ".png"))
plot(roc_curv, main=paste("ROC Curve (AUC=", round(auc_val, 3),")"), legacy.axes=TRUE, col='blue', lwd=3)
dev.off()
png(paste("roc_", task$tasks$type, "_", task$tasks$number, ".png", sep=""))
plot(roc_curv, main=paste("ROC Curve (AUC=", round(auc_val, 3),")"), legacy.axes=TRUE, col='blue', lwd=3)
dev.off()
png(paste("cal_", task$tasks$type, "_", task$tasks$number, ".png", sep=""))
calPerf = val.prob.ci.2(predictions, act2, logistic.cal=TRUE, smooth="none", col.log = "blue", col.smooth = "darkorange")
dev.off()
test_cal <- c("Intercept"=round(calPerf$Calibration$Intercept[[1]],2), "Slope"=round(calPerf$Calibration$Slope[[1]], 2))
test_cal
test_cal <- data.frame("Intercept"=round(calPerf$Calibration$Intercept[[1]],2), "Slope"=round(calPerf$Calibration$Slope[[1]], 2))
test_cal
png(paste("roc_", task$tasks$type, "_", task$tasks$number, ".png", sep=""))
#' This scripts fits the different models
#'
#'
#'
library(caret)
library(jsonlite)
library(yardstick)
library(pROC)
library(CalibrationCurves)
# library(dplyr)
split_data <- function(data) {
#' This function splits the data into a training and test set (70%,30%) and returns them as a list
set.seed(1)
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train <- data[sample, ]
test <- data[!sample, ]
return(list(train=train, test=test))
}
fit_task <- function(train, task) {
#' This function fits a linear model based on the task metadata description and returns the model
outcome_name <- task$outcome_variable
feature_names <- unlist(task$features)
task_formula <- reformulate(feature_names, outcome_name)
fit <- glm(task_formula, data=train, family = "binomial")
fit
}
evaluate_model <- function(fitted_model, train_data, test_data, task) {
#' Metrics: AUC, calibration
#' Plots: ROC-plot, calibration plot
#'
# Get the outcome variable
outcome_var <- task$outcome_variable
# Get the actual result
actual_train <- unlist(train_data$outcome_var)
actual_test <- unlist(test_data$outcome_var)
# Get the predicted probabilities of the model based on the data
probability_train <- predict(fitted_model, type="response")
probability_test <- predict(fitted_model, test_data, type="response")
# Create a confusion matrix plot
# conf_plot <- create_conf_plot(actual, probability)
# Plot the roc curves for the probabilities and the actual values
roc_curve_train <- roc(actual_train, probability_train, plot=FALSE)
roc_curve_test <- roc(actual_test, probability_test, plot=FALSE)
# Get the AUC values
auc_value_train <- auc(roc_curve_train)
auc_value_test <- auc(roc_curve_test)
# Save ROC plot and calibration plot in a .png file, store calibration values
save_roc_curve(roc_curve_train, roc_curve_test, task)
cal_values <- save_cal(probability_train, probability_test, actual_train, actual_test, task)
# Save variables: auc_value, intercept, slope
df_eval <- data.frame(dataset = task$full_name,
task = task$type,
test_environment = NULL,
AUC_train = auc_value_train,
AUC_test = auc_value_test,
calibration_train = cal_values$calibration_train,
calibration_test = cal_values$calibration_test,
N_train = NULL,
N_test = NULL,
num_features = NULL)
df_eval
}
# -- Is not necessary --
create_conf_plot <- function(actual, probability) {
#' For the confusion matrix, first get the predictions based on a threshold (0.5)
#' Create a dataframe of actual values and predicted values
#' Plot them in a graph
predictions <- ifelse(probability > 0.5, 1, 0)
conf_df <- as.data.frame.table(table(actual, predictions))
colnames(conf_df) <- c("Actual", "Predicted", "Count")
# Plot the confusion matrix for the threshold
conf_plot <- ggplot(data = conf_df, aes(x = Actual, y = Predicted, fill = Count)) +
geom_tile(color = "white") +
geom_text(aes(label = Count), vjust = 1) +
scale_fill_gradient(low = "hotpink", high = "lightgreen") +
theme_minimal() +
labs(x = "Actual", y = "Predicted", title = "Confusion Matrix")
}
save_roc_curve <- function(roc_curve_train, roc_curve_test, task) {
# Save the roc curve as a .png file
png(paste("roc_", task$type, "_", task$number, ".png", sep=""))
plot(roc_curve_train, main="ROC Curve", print.auc = TRUE, print.auc.y = 0.4, legacy.axes=TRUE, col='blue', lwd=2)
plot(roc_curve_test, legacy.axes=TRUE, col='purple', lwd=2, add=TRUE)
dev.off()
}
save_cal <- function(probability_train, probability_test, actual_train, actual_test, task) {
# Save the calibration curve as a .png file
png(paste("cal_train_", task$type, "_", task$number, ".png", sep=""))
cal_train = val.prob.ci.2(probability_train, actual_train, logistic.cal=TRUE, smooth="none", col.log = "blue")
dev.off()
png(paste("cal_test_", task$type, "_", task$number, ".png", sep=""))
cal_test = val.prob.ci.2(probability_test, actual_test, logistic.cal=TRUE, smooth="none", col.log = "purple")
dev.off()
incpt_train <- round(cal_train$Calibration$Intercept[[1]],2)
slope_train <- round(cal_train$Calibration$Slope[[1]],2)
incpt_test <- round(cal_test$Calibration$Intercept[[1]],2)
slope_test <- round(cal_test$Calibration$Slope[[1]],2)
cal_values <- data.frame("calibration_train" = c("Intercept"=incpt_train, "Slope"=slope_train),
"calibration_test" = c("Intercept"=incpt_train, "Slope"=slope_train)
)
cal_values
}
# Hierin gebeurt alles als het goed is
analysis <- function() {
# Get the processed data
path_processed_data <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/processed/processed_ari.csv'
processed_data <- read.csv(path_processed_data)
# Split the data into training and test set
split <- split_data(processed_data)
# Get the task from the metadata
path_metadata <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/metadata/ari.json'
metadata_ari <- fromJSON(path_metadata, simplifyVector=TRUE)
task_metadata <- metadata_ari$tasks # this is a list of tasks
# For each task, fit and evaluate the model
for (task in task_metadata) {
model <- fit_task(split$train, task)
evaluate_model(model, split$train, split$test, task)
}
}
# task <- fromJSON('{"tag": "ari",
#  "full_name": "ARI",
#  "url": "https://hbiostat.org/data/repo/ari.zip",
#  "tasks": [
#  {"number": "1",
#   "outcome_variable": "pneumonia_or_worse",
#   "type": "diagnosis",
#   "features": ["age", "rr"]
#  }
#  ]
#  }')
# Get the processed data
path_processed_data <- 'G:/.shortcut-targets-by-id/1wjZdCA5wyfmeZh1U0oi8m-SZT_PlkQb7/causal-casemix/causalcasemix/data/processed/processed_ari.csv'
processed_data <- read.csv(path_processed_data)
gc()
